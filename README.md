
## Problem/Example:
You have coins of denominations 1, 5, and 10, and you want to make 28 cents using the fewest coins.

**Step-by-step process:** 

- start with 28 cents. Use the largest coin (10 cents).
- Take two 10 cent coins. Remaining amount = 28 − 20 = 8 cents.
- Use the largest coin (5 cents): Take one 5 cent coin. Remaining amount = 8 − 5 = 3 cents.
- Use the largest coin (1 cent): Take three 1 cent coins. Remaining amount = 3 − 3 = 0 cents.

Coins used:
2 (10 cents) +1 (5 cents) + 3 (1 cent) = 6 coins total.


Students code the solution using a language of their choice


## Associated resources

_Decision trees in traditional computing_

In traditional computing, decision trees are often used as a flowchart-like structure that helps in decision-making processes based on a series of questions or criteria. They serve as decision-making tools, guiding users through a series of options to arrive at a conclusion or action based on predefined rules. 
Traditional decision trees usually rely on explicit rules set by domain experts. Each branch of the tree represents a decision point based on specific conditions. Once built, these trees do not adapt or learn from new data. They are static and based on fixed logic, which can limit their effectiveness in dynamic environments. Traditional decision trees are often simple and easy to understand but may struggle with complex decision-making scenarios where multiple criteria interact in non-linear ways. 
They are often used in business scenarios to guide users through processes, such as troubleshooting or customer support queries, where a series of yes/no questions lead to a resolution.

_Decision trees in machine learning_

In machine learning, decision trees are a form of predictive modelling that uses a tree structure to represent decisions and their possible consequences (outcomes). They aim to model complex relationships in data, making predictions or classifications based on input features. Machine learning decision trees are trained on historical data, meaning they learn patterns and relationships from the data rather than relying on predefined rules. These trees can adapt to new data when retrained, allowing them to improve their predictions over time. This dynamic aspect is crucial for handling real-world data variability. 
Machine learning decision trees can handle complex interactions between features and can model non-linear relationships, making them more powerful for predictive tasks. Techniques such as pruning are applied to prevent overfitting, ensuring that the decision tree generalises well to unseen data. Used in various fields such as finance for credit scoring, healthcare for patient diagnosis, and marketing for customer segmentation. They help in making predictions based on patterns in historical data.

## Video

https://www.youtube.com/watch?v=xDWZzD4TPO0

<a href="https://www.youtube.com/watch?v=xDWZzD4TPO0">
  <img width="554" height="306" alt="image" src="https://github.com/user-attachments/assets/c15a60ec-7719-4b3e-b882-4eab22f5d34b" />
</a>


## Other info
[Worksheet](https://docs.google.com/document/d/1c0MIvB2AMukDWGTeG8maJg_pK3zUZyiLl1fco2_GfXE/edit?usp=sharing)
_Software Engineering, 2025!_
